\documentclass[11pt, letterpaper, journal]{IEEEtran}

\usepackage[utf8]{inputenc}
\usepackage[letterpaper, margin=1.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[title]{appendix}
\usepackage{authblk}
\usepackage{cite}
\usepackage[font=scriptsize]{caption}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{multicol}
\usepackage{lipsum}
\usepackage{subfig}
\usepackage[dvipsnames]{xcolor}

% Some general setting
\graphicspath{ {./statics/} }
\captionsetup{justification=raggedright, singlelinecheck=false}


\title{Project 2: Arctic Cloud Detection}
\author[1]{Devin Ti}
\author[1]{Ryan Tang}
\affil[1]{Duke University, Statistical Science}

\date{December 6th 2022}

\begin{document}
\maketitle

\section{Introduction}
Global warming and how surface air temperatures changes has been a general scientific interest and public policy issue. In addition, many global climate studies predicted the global surface air temperature has the strongest correlation with the increase of the Arctic's atmospheric carbon dioxide level. Hence understanding how carbon dioxide changes in the Arctic is crucial to studying global warming. As the Arctic gets warmer, water vapors and changes in the distribution and proportionality of clouds can lead to further warming and spatial sensitivity, which indicates we need a systematic, accurate way of studying cloud distribution in the Arctic. However, such a study has its unique challenges. Primarily, the current algorithm not being well-suited for distinguishing clouds from ice particles because the two particles are similar in the lens of radiation measurements. MISR data offers a potential solution and introduces a sheer amount of data volume. But the currently deployed algorithm in MISR is not particularly targeted for detecting clouds over bright surfaces in polar regions. At the same time, computational constraints also limit the existing algorithm's performance. Therefore, coming up with a computationally efficient algorithm that delivers accurate detection is of the utmost importance.

MISR provides Arctic satellite images on each orbit on each path every 16 days interval. Each resulting data unit is an image concentrating on a particular, repeating Arctic location patch. Tao and other team members took the time to manually label the data unit with the aid of a labeling algorithm from Jet Propulsion Laboratory, which resulted in 71.5\% of conservative, expert label coverage at the pixel level for evaluating the algorithms. Furthermore, Tao's team introduced ELCM, a cloud detection algorithm based on the three expert-designed features using just if-else hard-coded rules, and utilized Quadratic Discriminant Analysis (QDA) for probabilistic prediction on top of ELCM labels that achieved astonishing results both in terms of precision and recall.

Tao's team introduced three expertly designed features through domain knowledge: CORR, SD, and NDAI. On a high level, these features were constructed through spatial convolution over a small grid under the knowledge that the aggregated radiation from multiple cameras is different between smoothed surfaces, ice and snow, and cloud. We should expect high CORR and small SD over clear or low-altitude cloud areas. NDAI provides a proxy to the visible wavelength where we should expect smaller, less volatile measures for ice and snow-covered surface than low-altitude clouds to further aid in distinguishing between the surface area from the low-altitude cloud.

Despite the amazing work done by Tao's team, constructing the cloud detection algorithm using a few hard-coded linear rules is not ideal. We can certainly do better by introducing a more sophisticated model for predicting cloud presence. Here, by utilizing the expert labels and the engineered features, we tested the performance on a few state-of-art methods and conducted an in-depth modeling calibration analysis for future potential improvements. 

\section{Exploration}
The dataset consists of 3 images shot at the same Arctic location from different orbits. Each consists of $305 \times 382$ pixels that are labeled, \{-1 = Not Cloudy, 0 = Not Sure, 1 = Cloudy\}. The former two categories are pixels that experts annotate, and the latter 'Unlabelled' category is pixels that were not. The labeling process was conservative, so pixels that are not totally obvious are left unlabelled. Along with an X and Y coordinate for each pixel, each pixel is associated with 8 covariates. In this section, we conduct initial exploratory data analysis, which greatly influenced our later modeling choices and testing-validation strategies.

\subsection{Label Dispersion}
\begin{figure*}[!h]
\centering
% \captionsetup{justification=centering}
\includegraphics[width=1.0\textwidth]{1.a.png}
\caption{Three data units taken at different times at the same Arctic location. Each pixel is color-coded by its respective expert label. Cloudy for \textcolor{BlueGreen}{blue}, \textcolor{Tan}{Brown} for surface, and White for unlabeled pixels.}
\label{fig:image_labels}
\end{figure*}

We first plot the spatial distribution of labels image-wise. This is shown in Figure \ref{fig:image_labels}, where we have plotted each of the three images overlaid with their associated class. As can be seen, especially in image 2 and image 3, many pixels are unlabelled. Further, the placement of labeled pixels also varies according to the images. This is further captured in Figure \ref{fig:label_dist}, which shows that the proportionality between the three classes varies significantly across images. Image 2 has a majority of points unlabelled, but image 1 has the majority of points labeled. Given the variations among the Satellite images, a reasonable classifier should be able to handle unbalanced classes while simultaneously generalizing well to a whole range of unseen Arctic satellite images --- one with absolutely no cloud or full cloud coverage.

\begin{figure}[!h]
\centering
% \captionsetup{justification=centering}
\includegraphics[width=0.48\textwidth]{2.a.png}
\caption{Label distribution percentages for the three images in our dataset}
\label{fig:label_dist}
\end{figure}

The dispersion of labels also shows how highly correlated the labels are to the spatial location of the images. In all three images, we can see large patches of pixels with the same label. This tells us that nearby pixels in the same image are likely to have the same labels. For example, if we predict by using the neighboring pixel, we can do extremely well due to data leakage. Hence, if we blindly do the model training using the typical randomized K-folds, the resulting scores are not that informative. In other words, the usual i.i.d samples assumption is inappropriate here; we must be particularly cautious about this in the training procedure.


\subsection{Covariate Signals}
We first investigate the distribution between the 3 expert engineered features, NDAI, CORR, and SD\footnote{SD has a much larger range than the rest; thus, we applied a log transformation for plotting purposes.}. Figure \ref{fig:covariate_pairplot} provides a pair plot of the 8 covariates, in either 1-D or 2-D KDE plots, that are available to us. We can see that all engineered features provide some signals from a different angle, and all the raw radiance measurements add little to no signal to the classification problem.

We notice a few interesting facts starting at the diagonal 1-D KDE plots. Most surface pixels have NDAI less than 0, CORR less than 0.25, and lower SD. NDAI alone provides an outstanding separability out-of-box. By plotting NDAI against SD, we see the two classes are already on their own distinct density mass with a slight overlap. The subplot in column 3 and row 1, CORR adds more information by stating that Cloud particles tend to have a higher spatial correlation. The surface particles have a low correlation across the spectrum, less than $0.25$. And the cloud particles, on the other hand, tend to have a higher correlation greater than $0.25$.

Next, we plot the features spatially with each pixel in its x-y coordinate overlaid with the three most influential features, NDAI, Corr, and Log(SD), in Figure \ref{fig:spatial_dist_covariates}. Firstly, we see that the values of the features are highly spatially correlated, meaning that neighboring pixels tend to have similar values. This is unsurprising since the expert features are derived based on a convolution kernel of either a 5x5 or 8x8 around the neighboring pixels. Interestingly we also observed that there is not one expert-engineered feature that perfectly aligns with all the labeled values in all the 3 images. That is to say that a high-performing classifier will likely need to use a combination of the features because a linear separating hyperplane does not exist.

\begin{figure*}[!h]
\centering
% \captionsetup{justification=centering}
\includegraphics[width=1.04\textwidth]{2.c.png}
\caption{Pair plot of all 8 features}
\label{fig:covariate_pairplot}
\end{figure*}

\begin{figure}[!h]
\centering
% \captionsetup{justification=centering}
\includegraphics[width=0.5\textwidth]{image_spatial_features.png}
\caption{Each image overlaid with values for NDAI, CORR, and Log(SD). Red represents high values and blue low values}
\label{fig:spatial_dist_covariates}
\end{figure}


\section{Preparation}

\subsection{Data Splitting}
Most out-of-box cross-validation (CV) methodology assumes i.i.d samples, which do not play well with the images that inherently contain spatial-temporal correlation. For instance, images can be taken at the same location but at different times, and neighboring pixels are highly correlated with each other. Image pixels are not i.i.d samples. If we are not careful with our training procedure, we can introduce unwarranted data leakage and unjustified scores. To alleviate the potential data leakage issue, we propose two specialized cross-validate-test (CVT) strategies to handle this spatial dependence shown in Figure \ref{fig:test_schema}.

\begin{figure*}
    \centering
    \subfloat[\centering Test Schema 1]{{ \includegraphics[width=0.5\textwidth]{test_scheme1.png} }}
    \subfloat[\centering Test Schema 2]{{ \includegraphics[width=0.5\textwidth]{test_scheme2.png} }}
    \caption{Two different CVT methods. \textcolor{Gray}{Gray} patches represent training data, \textcolor{orange}{orange} patches represent validation data, and \textcolor{ForestGreen}{green} patches represent test data.}%
    \label{fig:test_schema}%
\end{figure*}

\subsubsection{CVT Scheme 1}
With only 3 image units given, we first keep image 1, the one with most of the pixels labeled, out for our testing set, then use the other 2 images for the training and validation set. The procedure is illustrated in the left plot of Figure \ref{fig:test_schema}. In other words, we train and tune our models using only 2 images and test on the last image. Then, we pick a $55\times55$ kernel size that defines our validation set, shown in \textcolor{orange}{orange}, and remove the surrounding pixels of the kernel completely from the training set, shown in the white spaces. If used in training, the white region represents omitted data points, as these points will contain information about the pixels in the validation/test set. Then, the remaining gray area is the training set. Note this CV procedure is applied on the 2 training images simultaneously and at the same patch. It is important because taking validation patches at different areas can potentially introduce leakage if the two images were shot at the same location. Lastly, the validation patch shifts around the image, with each shift forming a cross-validation (CV) fold. Although it is not the best use of limited data, it ensures there is no data leakage through spatial dependencies. 

\subsubsection{CVT Scheme 2}
The validation approach remains the same in these strategies, but the testing method differs in the second Scheme. We introduce a nested approach to make the most use of the limited data. In other words, we can train, validate, and test each at the same time each fold instead of keeping one testing image out completely. To illustrate, the right plot of Figure \ref{fig:test_schema} contains an example. With this procedure, we can use all 3 images as a combined training, CV, and test set. Here we have an additional \textcolor{ForestGreen}{green} patch representing our test set only with the \textcolor{orange}{orange} validation set. We segment a part of the image at each fold and use it in testing. While both patches shift around the image, everything else stays the same, 

\subsection{Baseline}
We compute the accuracy of a trivial classifier for both test schemes, which sets all labels to cloud-free. These results are reported in Table \ref{tab:scheme1_results}. We see that a naive classifier that predicts all pixels are surface achieves around 52\% accuracy, which is well underperforming from the rest of the classifiers. Such exercise sets the baseline that the result we achieve here is not trivial.

\subsection{First Order Importance}
In section 2.B, we extensively covered the importance of features, which is also shown in Figure \ref{fig:covariate_pairplot}. We can achieve pretty far on the classification task by only using the expert-engineered features, NDAI, CORR, and SD. We can see in the KDE plot of NDAI and SD that surface and cloud pixels form distinct density mass with a slight overlap. Adding CORR into the mix, by identifying pixels with low correlation are typically surface pixels, we can achieve a reasonable result that should remember like a logistic regression using only these 3 features.


\section{Modeling}
\subsection{Models Used and Assumptions}
We try 4 classification methods, logistic regression (L2 regularized), CART, Random Forest, and Adaboost. Firstly we provide some brief commentary on the assumptions of these models.
\begin{itemize}
    \item \underline{Logistic Regression}. This model is linear and assumes a linear decision boundary between the two classes, from our plots the data seems well separated linearly by most of the features, however we cannot be sure in the high dimensional space.
    \item \underline{CART}. This model is non-linear but partitions the feature space into rectangles. In particular it will generally perform worse if the decision boundary is not axis aligned. 
    \item \underline{Random Forest}. This model is non-linear and uses bootstrap aggregation to reduce variance. Another point that is important is that the model works better when the set of trees trained is less correlated, one factor that affects this is if different covariates used are correlated. This is something that affects us since we use all 7 features, but we know that 3 of the features (NDAI, CORR and SD) are actually transforms of the remaining 4.
    \item \underline{Adaboost}. This model is also non-linear and uses boosting to overcome limitations of a single weak classifier. In this case we use a pruned CART tree as the weak learner. There are not many assumptions of this model that affect our use of it.
\end{itemize}
\\
Lastly, all these models assume that input data is IID. We know that they are not due to the spatial dependence. As mentioned we have tried to account for this in our testing and validation procedure. 

\subsection{CV Results}
We use patch sizes of 55x55. In total this produces 44 folds for cross validation for test scheme 1. And 40 folds for cross-validation/testing in test scheme 2. We consider two metrics, accuracy and balanced accuracy for computing the cross validated error and test error for both test schemes. 

% Results 1 table
\begin{table}
\begin{center}
\resizebox{\columnwidth}{!}{
\begin{tabular}{||c c c c c||} 
    \hline
    Model & CV Acc. & CV B.Acc. & Test Acc. & Test B.Acc. \\ [0.5ex] 
    \hline\hline
    LR & 0.880 & 0.869 & 0.769 & 0.764 \\ 
    \hline
    RF& 0.881 & 0.876 & 0.494 & 0.645 \\
    \hline
    CART & 0.872 & 0.868 & 0.759 & 0.749 \\
    \hline
    Adaboost & 0.884 & 0.874 & 0.788 & 0.778 \\
    \hline
\end{tabular}
}
\caption{CV and test results for the 4 models under test scheme 1. "B.Acc" stands for balanced accuracy.}
\label{tab:scheme1_results}
\end{center}
\end{table}

% Results 2 Table
\begin{table}
\begin{center}
\resizebox{\columnwidth}{!}{
\begin{tabular}{||c c c c c||} 
\hline
Model & CV Acc. & CV B.Acc. & Test Acc. & Test B.Acc. \\ [0.5ex] 
\hline\hline
LR & 0.877 & 0.820 & 0.877 & 0.820 \\ 
\hline
RF& 0.878 & 0.819 & 0.877 & 0.814 \\
\hline
CART & 0.860 & 0.821 & 0.860 & 0.821 \\
\hline
Adaboost & 0.867 & 0.805 & 0.867 & 0.805 \\
\hline
\end{tabular}
}
\caption{CV and test set results for the 4 models under test scheme 2}
\label{tab:scheme2_results}
\end{center}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{statics/test_Scheme_1_Fold_error.png}
\caption{Accuracy and Balanced Accuracy for each validation fold in Test Scheme 1. Top left: Logistic Regression, Top Right: Random Foret, Bottom Left: CART, Bottom Right: Adaboost}
\label{fig:fold_results_1}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{statics/test_Scheme2_fold_error.png}
\caption{Accuracy and Balanced Accuracy for each validation fold in Test Scheme 2. Top left: Logistic Regression, Top Right: Random Foret, Bottom Left: CART, Bottom Right: Adaboost}
\label{fig:fold_results_2}
\end{figure}

\\
Aggregated validation and test results are provided in Tables \ref{tab:scheme1_results} and \ref{tab:scheme2_results} for test scheme 1 and 2 respectively. \textcolor{red}{Add commentary about model results}.
\\
These tables also show the difference in test scheme 1 and test scheme 2. We can see that the CV results and the test results are very similar in test scheme 2 (as seen from Table \ref{tab:scheme2_results}), whereas the test results in Table \ref{tab:scheme1_results} are generally worse than the CV results. This reflects the fact that in test scheme 2, the nested test - validation procedure means that the validation and testing procedure at the exact same, hence we can expect that both produce similar errors. In contrast with test scheme 1, the test procedure (testing on 1 image) is quite different from the validation procedure. We see that test scheme 1 allows us to detect over fitting of some models even when high CV accuracy is shown. This is the case in the Random Forest model, which had the second best performing CV accuracy, but the poorest test accuracy.
\\
More insights can be gleaned from Figures \ref{fig:fold_results_1} and \ref{fig:fold_results_2} which plots the individual fold errors. Of key note here is that the errors fluctuate a fair bit over the different folds. This is due to our patch wise formation of the validation set, meaning that the labels in most of the patches are going to be similar. In subsequent sections we analyse whether our model systematically makes errors on certain labels. 



\subsection{ROC Plots}
We plot the ROC curves generated for both the validation and test set(s) used in testing scheme 1 and testing scheme 2. These are displayed in \ref{fig:ROC_curves scheme1} and \ref{fig:ROC_curves scheme2}. An ROC curve highlights the predicted true positive rate a model can achieve for a given false positive rate (or vice versa). (Insert explanation about which do we care more about).
\\ 
Intuitively a model that has a more favourable trade off between the two is better performing, this can be measured by the area under curve metric. As can be seen the AUC between LR, RF and Adaboost is similar across both testing schemes and across both the validation and testing sets. One surprising exception to this is the random forest model, which sees a sharp drop in AUC in testing scheme 1 on the test set. This is likely indicative of overfitting in the RF model in test scheme 1. The CART model seems to perform the worse, as highlighted by its consistently lower AUC across both validation / testing sets.

\begin{figure}%
    \centering
    \subfloat[\centering Test Scheme 1: CV ROC]{{\includegraphics[width=8.5cm]{statics/test_Scheme1_cv_roc.png} }}%
    \qquad
    \subfloat[\centering Test Schema 1: Test ROC]{{\includegraphics[width=8.5cm]{statics/test_Scheme1_test_roc.png} }}%
    \caption{Left: ROC curves per fold and Mean ROC curve for the 4 models. Right: ROC curves of the single held out test set for the 4 models. }%
    \label{fig:ROC_curves scheme1}%
\end{figure}

\begin{figure}%
    \centering
    \subfloat[\centering Test Scheme 2: CV ROC]{{\includegraphics[width=8.5cm]{statics/test_Scheme2_cv_roc.png} }}%
    \qquad
    \subfloat[\centering Test Schema 2: Nested Test ROC]{{\includegraphics[width=8.5cm]{statics/test_Scheme2_test_roc.png} }}%
    \caption{Left: ROC curves per fold and Mean ROC curve for the 4 models. Right: ROC curves of each test fold and Mean ROC curves across all test folds for the 4 models. }%
    \label{fig:ROC_curves scheme2}%
\end{figure}


\section{Diagnostics}
We base on our choice of model on performance on testing scheme 1. This is because the testing scheme better reflects the actual use case of the model, where inference will be made on whole images. Hence we choose the the Adaboost model which has the best validation and test performance. 

\subsection{Analysis of Adaboost}
To analyse the converge we look at the sequence of error rate generated by the boosting alrogithm. We also analyse the feature importance to check for stability in the chosen features over the various validation folds and train set.
\subsubsection{Convergence Analysis}
Being an iterative algorithm, we first assess the performance of the adaboost learner across each iteration. This is displayed in \ref{fig:Adaboost_iterations}. We see that the test error of the algorithm converges, starting to rise after initially decreasing after just 25 boosting iterations. More boosting iterations are likely to result in over fitting of the training set. We believe that the algorithm converges with so little boosting iterations due to the expert labelled features which provide a large amount of information to the model. As can be see even the initial decision stump is able to achieve close to 12.4\% train error.

\begin{figure}[h]
\centering
\includegraphics[width=0.52\textwidth]{statics/ada_iterations.png}
\caption{Percentage error across subsequent iterations of Adaboost. Left: Error on Train Set, Right: Error on Test Set.}
\label{fig:Adaboost_iterations}
\end{figure}


\subsubsection{Feature Importance}
Next we assess the stability of the adaboost model via its learnt feature importance. This feature importance scored is calculated by aggregating, for a single run the feature importance of each weak learner (decision tree) learnt by the algorithm. We calculate feature importance in test scheme 1 for 1) Model trained on the full train/validation set 2) Model trained at each cross validation fold. These results are shown in Figure \ref{fig:Feature_importance ts1}.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{statics/Feature_importance_test_scheme1.png}
\caption{Feature Importance of Adaboost for 1) model trained on full train and validation set. 2) Each CV fold 3) Average across CV folds.}
\label{fig:Feature_importance ts1}
\end{figure}

\\
Comparing the learnt feature importance we see that the model learnt on the full training set generally matches those learnt in the cross validation models. For example, NDAI is consistently the highest ranked feature. Likewise Corr is ranked highly in both the testing model and the cross validation models. This similarity in feature importance indicates model stability across the various data splits and with the final test model. 

\subsection{Analysing Error Patterns}
We analyse error patterns mainly by looking at the predicted probabilities across different spatial regions as well as different regions in the feature space. The latter poses challenges due to the high dimensional nature of the feature space. We describe in subsequent sections how we overcame this using dimensionality reduction.
\subsubsection{Prediction Probabilities}
Next we investigate how well calibrated the probabilities generated by Adaboost is. This will give us insights into have the algorithm performs. For each image, we generate predictions of the probability of a cloud prediction for each pixel. We then compare this to the true expert labelled pixels. These are plotted in Figure \ref{fig:Probability_Preds}.

\begin{figure}%
    \centering
    \subfloat[\centering Test Image 1]{{\includegraphics[width=9.3cm]{statics/ada_test_prob.png} }}%
    \qquad
    \subfloat[\centering Train Image 1]{{\includegraphics[width=9.3cm]{statics/ada_train_prob1.png} }}%
    \qquad
    \subfloat[\centering Train Image 2]{{\includegraphics[width=9.3cm]{statics/ada_train_prob2.png} }}%
    \caption{Left Column: Pixels colored by expert labels, Red: Clouds, Blue: No Clouds, White: Unlabelled. Right Column: Predicted probability given by the model.}%
    \label{fig:Probability_Preds}%
\end{figure}
\\
Firstly for the training case, we can see clearly that across image 1 and 2, there are more non-cloud labelled points than cloud points. This create a bias in the training data. Interestingly we see that the model seems to be biased towards predicting a pixel as a cloud. With large swaths of the image's predicted probability being slightly greater than 0.5. For example in the first training image, we see mis-classification occurs in non-cloud regions being predicted as clouds in areas where the non-cloud regions are surrounded by clouds. The same is seen in the second training image, where only the right most patch of non-cloud regions is successfully predicted by the adaboost algorithm. 
\\
Another point to note is that the model is generally very cautious about its probability prediction for the cloud class, with the highest predicted probability being only 0.61. In contrast, it is more confident about predicting a non-cloud, with predictive probabilities reaching 0.113. This is reflected in Figure \ref{fig:Probability_Preds}, where the red probability regions are very faint, and the blue probability regions are darker. This is a positive feature of the model since the train set generally has less cloud labelled points, hence it is natural that the model should generally be less confident about predicting points as the cloud class. In contrast, since the dataset has relatively more non-cloud class, the model has become more confident when predicting them. Thus we have reason to believe that given more training data, the model will become more confident about its cloud class prediction.

\subsubsection{Decision Surface Analysis}
Next we analyze the decision surface of Adaboost. This is a similar analysis to that in the previous section, except that now instead of analyzing predicted probabilities with respect to the spatial coordinates of the pixel, we do so with respect to the features used. 
\\
\\
There are in total 7 features used, meaning that the decision surface cannot be easily visualized due to the high dimension. We thus conduct a PCA first, projecting the feature space into 2 dimensions. This learnt PCA can then used to inversely project a grid of 2 dimension points to the covariate space which we can then classify using the model. Doing this we obtain can obtain a low - diemsional representation of the decision surface our model. This is represented in Figure \ref{fig:decision_surface}. \\

\begin{figure}%
    \centering
    \subfloat[\centering Low Dimension Representation of Adaboost Decision Surface]{{\includegraphics[width=7cm]{statics/PCA_decision_surface.png} }}%
    \qquad
    \subfloat[\centering Low Dimension Representation of Logistic Regression Decision Surface]{{\includegraphics[width=7cm]{statics/log_reg_decision_surface.png} }}%

    \caption{Decision Surfaces of Adaboost (Top) and Logistic Regression (Bottom). }%
    \label{fig:decision_surface}%
\end{figure}

For comparison we have plotted the decision surface of logistic regression model. Once again we can see a similar trend as in Figure \ref{fig:Probability_Preds}. The model tends to be more confident in the non cloud (blue) prediction versus the cloud (red) prediction. We further see a more complex decision surface where regions with mostly cloud points have high classification probability. As the space becomes mixed with cloud and non cloud points the confidence in classifying as non-clouds steadily decreases and starts to shift towards classifying as clouds as the we move into the space populated mainly by cloud points. Interestingly, as we move further towards the top right of the 2D space the model actually becomes less confident about its predictions. Contrast this to the decision surface generated by the logistic regression. In this case the decision surface is almost uniformly confident throughout in the two regions of its decision surface, it is exhibits very high confidence in regions where there is little data, or where there is a lot of overlapping data from both classes.\\

Combining the decision surface and the pixel wise probabilities in Figure \ref{fig:Probability_Preds}. We conclude that the Adaboost algorithm is able to learn sensible probabilities. This accurate quantification of uncertainty by the Adaboost algorithm is important given the small amount of data available in the problem. Further, we see that as more data is given Adaboost becomes more confident in its prediction, meaning that it has enough capacity to learn even more complex decision surfaces given more data. 


\subsection{Alternative Classifiers}
There are two key issues with our model. Firstly, our model ignores spatial dependencies between pixels; secondly, it is a supervised learning model that requires expert labels to train.
\subsubsection{Spatial Dependencies}
Several models can explicitly account for spatial dependencies in images. One example would be the convolutional neural network, which we could train to produce pixel-wise outputs of the cloud prediction based on the input features (stacked as an image). Another option that doesn't require expert labeling is the Ising / Potts model, which explicitly encodes the spatial dependencies via an undirected graph. These two models should perform better than our selected classifiers by directly modeling the innate spatial dependencies of the image data.
\subsubsection{Limited Data Size}
As said, our algorithm requires expert labels to train, which is often time-consuming and not easily obtainable. The main concern is the relatively small set of images we have for training. As such, we will likely encounter future test images of highly different covariate distributions (data drift) and also different covariate-to-output relations (concept drift). The currently trained classifier will perform poorly on these edge cases. As such, unless we have a bigger training dataset, we are not confident of the model's ability to generalize to a large set of new images. 

\subsection{Conclusion/Why Adaboost}
\begin{itemize}
    \item We have shown that Adaboost is able to learn well calibrated decision surfaces, despite the relative lack of data.
    \item Hence we are confident this algorithm will work well given a larger set of images.
    \item The ability to generated predicted probabilities is also useful in labelling new images in the future, which we could do so via an active learning procedure.
\end{itemize}


\section{Conclusion}


\section{Appendix}


\end{document}